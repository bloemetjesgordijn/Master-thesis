{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf34d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('models/configured_spacy_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0306ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_file = open('saves/trafficking_df.pkl', \"rb\")\n",
    "trafficking_df = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c7f8d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load country configs & country classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb957b5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "open_file = open('saves/country_translation_dict.pkl', \"rb\")\n",
    "country_translation_dict = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce55ea9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "open_file = open('saves/countries_to_exclude.pkl', \"rb\")\n",
    "countries_to_exclude = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c5ef3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyCcbIhMxSz5OP74pDT0aQLTvXDSMaV8tFk'\n",
    "geocode_url = 'https://maps.googleapis.com/maps/api/geocode/json?address='\n",
    "\n",
    "def get_geocode_country(txt):\n",
    "    res = requests.get(f\"{geocode_url}{txt}&key={api_key}\").json()['results']\n",
    "    country_name = \"None\"\n",
    "    try:\n",
    "        for address_component in res[0]['address_components']:\n",
    "            if 'country' in address_component['types']:\n",
    "                country_name = address_component['long_name']\n",
    "    except:\n",
    "        return \"None\"\n",
    "    return country_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6b599",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "\n",
    "def get_geopy_country(txt):\n",
    "    try:\n",
    "        location = geolocator.geocode(txt, language='en')\n",
    "        country_name = location.raw['display_name'].split(',')[-1].strip()\n",
    "        return country_name\n",
    "    except:\n",
    "        return \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34cce05",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def country_pipeline(txt):\n",
    "    txt = txt.lower()\n",
    "    # Check if already exists\n",
    "    if txt in countries_to_exclude:\n",
    "        return \"None\"\n",
    "    for key in country_translation_dict:\n",
    "        if txt in country_translation_dict[key]:\n",
    "            return key\n",
    "    \n",
    "    # Get location\n",
    "    geopy_loc = get_geopy_country(txt)\n",
    "    if geopy_loc == \"None\":\n",
    "        countries_to_exclude.append(txt)\n",
    "        return \"None\"\n",
    "    else:\n",
    "        geocode_loc = get_geocode_country(txt)\n",
    "        if geocode_loc == \"None\":\n",
    "            countries_to_exclude.append(txt)\n",
    "            geopy_mistakes[txt] = geopy_loc\n",
    "            return \"None\"\n",
    "        else:\n",
    "            if geocode_loc not in country_translation_dict:\n",
    "                country_translation_dict[geocode_loc] = []\n",
    "            country_translation_dict[geocode_loc].append(txt)\n",
    "            return geocode_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137987ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "country_pipeline('mek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84cf77",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02b27caf",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7023f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get linguistic distance between token a and token b. After iter 10 it is deemed a too far distance.\n",
    "def get_linguistic_distance(a, b):\n",
    "    tokens_to_consider = [b]\n",
    "    found = False\n",
    "    iters = 0\n",
    "    while not found:\n",
    "        for token in tokens_to_consider:\n",
    "            tokens_to_add = []\n",
    "            for ancestor in token.ancestors:\n",
    "                if ancestor not in tokens_to_add and ancestor not in tokens_to_consider:\n",
    "                    tokens_to_add.append(ancestor)\n",
    "            for child in token.children:\n",
    "                if child not in tokens_to_add and child not in tokens_to_consider:\n",
    "                    tokens_to_add.append(child)\n",
    "            tokens_to_consider = tokens_to_consider + tokens_to_add\n",
    "        for x in tokens_to_consider:\n",
    "            if a.orth == x.orth:\n",
    "                found = True\n",
    "        iters += 1\n",
    "        if iters == 10:\n",
    "            found = True\n",
    "    return iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef16eca3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_adposition_from_loc(token):\n",
    "    for child in token.children:\n",
    "        if child.pos_ == \"ADP\" and child.dep_ == \"case\":\n",
    "            return child.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e6d94",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_chunk_info_without_log(txt):\n",
    "    result = {}\n",
    "    for token in nlp(txt):\n",
    "        if token.ent_type_ == \"DRUG\":\n",
    "            drug_info = extract_info_from_drug_without_log(token, txt)\n",
    "            result[token.text] = drug_info\n",
    "    return result\n",
    "            \n",
    "adj_list = []          \n",
    "def extract_info_from_drug_without_log(drug, txt):\n",
    "    volumes = []\n",
    "    locations = {}\n",
    "    irrelevant_locations = []\n",
    "    for token in nlp(txt):\n",
    "        # Extract countries\n",
    "        if token.ent_type_ == \"GPE\":\n",
    "            dist = get_linguistic_distance(drug, token)\n",
    "            adj = get_adposition_from_loc(token)\n",
    "            locs = [token.text]\n",
    "            for loc in token.conjuncts:\n",
    "                locs.append(loc.text)\n",
    "            if adj not in locations:\n",
    "                locations[adj] = locs\n",
    "                if adj not in adj_list:\n",
    "                    adj_list.append(adj)\n",
    "            else:\n",
    "                for loc in locs:\n",
    "                    if loc not in locations[adj]:\n",
    "                        locations[adj].append(loc)\n",
    "        \n",
    "        # Extract volume\n",
    "        if token.ent_type_ == \"QUANTITY\":\n",
    "            volume = {}\n",
    "            dist = get_linguistic_distance(drug, token)\n",
    "            second_token = \"\"\n",
    "            quantity = {}\n",
    "            for ancestor in token.ancestors:\n",
    "                if ancestor.ent_type_ == \"QUANTITY\":\n",
    "                    second_token = ancestor\n",
    "            for child in token.children:\n",
    "                if child.ent_type_ == \"QUANTITY\":\n",
    "                    second_token = child\n",
    "\n",
    "            ## Decide volume and volume_type\n",
    "            if not isinstance(second_token, str):\n",
    "                if nlp(token.text)[0].ent_type_ == \"CARDINAL\":\n",
    "                    volume['volume'] = token.text\n",
    "                    volume['volume_type'] = second_token.text\n",
    "                    volume['dist'] = dist\n",
    "                elif nlp(second_token.text)[0].ent_type_ == \"CARDINAL\":\n",
    "                    volume['volume'] = second_token.text\n",
    "                    volume['volume_type'] = token.text\n",
    "                    volume['dist'] = dist\n",
    "\n",
    "                #Only append when not already in volumes\n",
    "                if volume not in volumes:\n",
    "                    volumes.append(volume)\n",
    "                \n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    result = {}\n",
    "    if bool(locations):\n",
    "        result['locations'] = locations\n",
    "    result[\"volume\"] = volumes\n",
    "    if len(irrelevant_locations) > 0:\n",
    "        result[\"irrelevant_locations\"] = irrelevant_locations\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026ce90",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4313e59f",
   "metadata": {},
   "source": [
    "### Fusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_chunks(chunks): \n",
    "    # Loop through chunks\n",
    "    drug_dict = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_info = extract_chunk_info_without_log(chunk)\n",
    "        \n",
    "        # Loop through drugs\n",
    "        if chunk_info is not None:\n",
    "            for drug in chunk_info:\n",
    "                lowerdrug = drug.lower()\n",
    "                if lowerdrug not in drug_dict:\n",
    "                    drug_dict[lowerdrug] = {'locations': [], 'volumes': []}\n",
    "                if len(chunk_info[drug]['volume']) > 0:\n",
    "                    drug_dict[lowerdrug]['volumes'].append(chunk_info[drug]['volume'])\n",
    "                if 'locations' in chunk_info[drug]:\n",
    "                    drug_dict[lowerdrug]['locations'].append(chunk_info[drug]['locations'])\n",
    "    return drug_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_locations(fused_chunks):\n",
    "    for drug in fused_chunks:\n",
    "        adjectives = {}\n",
    "        for location_entry in fused_chunks[drug]['locations']:\n",
    "            for adjective in location_entry:\n",
    "                if adjective not in adjectives:\n",
    "                    adjectives[adjective] = []\n",
    "                for country in location_entry[adjective]:\n",
    "                    if country not in adjectives[adjective]:\n",
    "                        adjectives[adjective].append(country)\n",
    "        fused_chunks[drug]['locations'] = adjectives\n",
    "    return fused_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ec850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_volumes(fused_locations):\n",
    "    for drug in fused_locations:\n",
    "        final_volume = {}\n",
    "        dist = 100\n",
    "        for volumes in fused_locations[drug]['volumes']:\n",
    "            if len(volumes) > 0:\n",
    "                for volume in volumes:\n",
    "                    if 'dist' in volume:\n",
    "                        if volume['dist'] < dist:\n",
    "                            dist = volume['dist']\n",
    "                            final_volume = volume\n",
    "        fused_locations[drug]['volumes'] = final_volume\n",
    "        return fused_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feed4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_locations(data):\n",
    "    geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "    if data is not None:\n",
    "        for drug in data:\n",
    "            if 'locations' in data[drug]:\n",
    "                data[drug]['original_locations'] = data[drug]['locations'].copy()\n",
    "                for adjective in data[drug]['locations']:\n",
    "                    country_list = []\n",
    "                    locations = data[drug]['locations'][adjective]\n",
    "                    for loc in locations:\n",
    "                        country = country_pipeline(loc)\n",
    "                        if country != \"None\":\n",
    "                            if country not in country_list:\n",
    "                                country_list.append(country)\n",
    "                    data[drug]['locations'][adjective] = country_list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efa26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_directions(data):\n",
    "    from_adjectives = ['uit', 'vanuit', 'van']\n",
    "    to_adjectives = ['naar']\n",
    "    via_adjectives = ['via']\n",
    "    if data is not None:\n",
    "        for drug in data:\n",
    "            fromlocs = []\n",
    "            tolocs = []\n",
    "            vialocs = []\n",
    "            locations = data[drug]['locations']\n",
    "            for adj in locations:\n",
    "                if adj in from_adjectives:\n",
    "                    for loc in locations[adj]:\n",
    "                        if loc not in fromlocs:\n",
    "                            fromlocs.append(loc)\n",
    "                elif adj in to_adjectives:\n",
    "                    for loc in locations[adj]:\n",
    "                        if loc not in tolocs:\n",
    "                            tolocs.append(loc)\n",
    "                elif adj in via_adjectives:\n",
    "                    for loc in locations[adj]:\n",
    "                        if loc not in vialocs:\n",
    "                            vialocs.append(loc)\n",
    "\n",
    "            data[drug]['locations'] = {'from': fromlocs, 'to': tolocs, 'via': vialocs}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777f298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f07c3cf3",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(columns=['ecli', 'drug', 'relevant_countries', 'volume', 'volume_type'])    \n",
    "\n",
    "    \n",
    "                            \n",
    "for index, row in trafficking_df.iterrows():\n",
    "    id = row['ecli']\n",
    "    \n",
    "    fused_chunks = fuse_chunks(row['chunks'])\n",
    "    fused_locations = fuse_locations(fused_chunks)\n",
    "    fused_volumes = fuse_volumes(fused_locations)\n",
    "    translated_locations = translate_locations(fused_volumes)\n",
    "    location_directions = get_location_directions(translated_locations)\n",
    "#     print(location_directions)\n",
    "    \n",
    "\n",
    "    if translated_locations is not None:\n",
    "        for drug in translated_locations:\n",
    "            relevant_countries = []\n",
    "            \n",
    "            curr = translated_locations[drug]\n",
    "            for adjective in curr['locations']:\n",
    "                locs = curr['locations'][adjective]\n",
    "                for loc in locs:\n",
    "                    if loc not in relevant_countries:\n",
    "                        relevant_countries.append(loc)\n",
    "                        \n",
    "            volumes = \"None\"\n",
    "            if len(curr['volumes']) > 0:\n",
    "                if isinstance(curr['volumes'], list):\n",
    "                    volumes = curr['volumes'][0][0]\n",
    "                else:\n",
    "                    volumes = curr['volumes']\n",
    "                if len(volumes) == 3:\n",
    "                    row = {'ecli': id, 'drug': drug, 'relevant_countries': relevant_countries, 'volume': volumes['volume'], 'volume_type': volumes['volume_type']}\n",
    "                else:\n",
    "                    row = {'ecli': id, 'drug': drug, 'relevant_countries': relevant_countries, 'volume': 'N/A', 'volume_type': 'N/A'}\n",
    "            else:\n",
    "                row = {'ecli': id, 'drug': drug, 'relevant_countries': relevant_countries, 'volume': 'N/A', 'volume_type': 'N/A'}\n",
    "                \n",
    "                \n",
    "            final_df = final_df.append(row, ignore_index=True)\n",
    "        \n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c4d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_pickle(\"final_df.pkl\")  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
