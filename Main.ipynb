{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file DS_Store\n"
     ]
    }
   ],
   "source": [
    "dataPath = os.getcwd() + '/court case data/testdata/'\n",
    "caseCount = len(os.listdir(dataPath))\n",
    "data = []\n",
    "try:\n",
    "    os.remove(dataPath + \".DS_Store\")\n",
    "except:\n",
    "    print(\"No file DS_Store\")\n",
    "for filename in os.listdir(dataPath):\n",
    "    f = open(os.path.join(dataPath, filename), encoding='utf-8')\n",
    "    data.append([filename.replace('.txt', ''), f.read()])\n",
    "\n",
    "verdict_df = pd.DataFrame(data, columns=[\"id\", \"case text\"])\n",
    "cases_df = pd.read_csv('./court case data/testdata.csv')\n",
    "merged_df = cases_df.join(verdict_df.set_index('id'), on='id', how='left')\n",
    "\n",
    "merged_df[\"verdict_date\"] = pd.to_datetime(merged_df[\"verdict_date\"])\n",
    "merged_df[\"publication_date\"] = pd.to_datetime(merged_df[\"publication_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>verdict_date</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>verdict_type</th>\n",
       "      <th>jurisdiction_type</th>\n",
       "      <th>case text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ECLI-NL-RBNNE-2021-5018</td>\n",
       "      <td>2021-01-10</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>uitspraak</td>\n",
       "      <td>['Strafrecht']</td>\n",
       "      <td>\\n\\nRECHTBANK NOORD-NEDERLAND\\nAfdeling strafr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECLI-NL-RBZUT-2003-AH9598</td>\n",
       "      <td>2003-03-06</td>\n",
       "      <td>2003-09-07</td>\n",
       "      <td>uitspraak</td>\n",
       "      <td>['Strafrecht']</td>\n",
       "      <td>\\n\\nRECHTBANK ZUTPHEN\\nMeervoudige economische...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ECLI-NL-RBZWB-2020-2646</td>\n",
       "      <td>2020-06-23</td>\n",
       "      <td>2020-06-23</td>\n",
       "      <td>uitspraak</td>\n",
       "      <td>['Strafrecht']</td>\n",
       "      <td>\\n\\nRECHTBANK ZEELAND-WEST-BRABANT\\n\\nStrafrec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ECLI-NL-GHAMS-2019-1601</td>\n",
       "      <td>2019-08-05</td>\n",
       "      <td>2019-07-26</td>\n",
       "      <td>uitspraak</td>\n",
       "      <td>['Strafrecht']</td>\n",
       "      <td>\\n\\nafdeling strafrecht\\nparketnummer: 23-0017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ECLI-NL-GHAMS-2019-1602</td>\n",
       "      <td>2019-08-05</td>\n",
       "      <td>2019-07-26</td>\n",
       "      <td>uitspraak</td>\n",
       "      <td>['Strafrecht']</td>\n",
       "      <td>\\n\\nafdeling strafrecht\\nparketnummer: 23-0017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18119</th>\n",
       "      <td>ECLI-NL-RBAMS-2013-1294</td>\n",
       "      <td>2013-01-29</td>\n",
       "      <td>2013-08-10</td>\n",
       "      <td>uitspraak</td>\n",
       "      <td>['Strafrecht']</td>\n",
       "      <td>\\n\\nRECHTBANK AMSTERDAM\\n\\n\\nVONNIS\\n\\n \\n\\n13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18120</th>\n",
       "      <td>ECLI-NL-PHR-2020-1106</td>\n",
       "      <td>2020-11-24</td>\n",
       "      <td>2020-11-24</td>\n",
       "      <td>conclusie</td>\n",
       "      <td>['Strafrecht']</td>\n",
       "      <td>\\n\\nPROCUREUR-GENERAAL\\n\\n\\nBIJ DE\\n\\n\\nHOGE R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18121</th>\n",
       "      <td>ECLI-NL-GHAMS-2017-2618</td>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>2017-05-07</td>\n",
       "      <td>uitspraak</td>\n",
       "      <td>['Strafrecht']</td>\n",
       "      <td>\\n\\n\\nparketnummer: 23-001217-13\\ndatum uitspr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18122</th>\n",
       "      <td>ECLI-NL-RBAMS-2013-BZ0392</td>\n",
       "      <td>2013-01-29</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>uitspraak</td>\n",
       "      <td>['Strafrecht']</td>\n",
       "      <td>\\nRECHTBANK AMSTERDAM \\nVONNIS  \\n\\n13/529144-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18123</th>\n",
       "      <td>ECLI-NL-GHAMS-2007-BB2447</td>\n",
       "      <td>2007-07-13</td>\n",
       "      <td>2007-08-29</td>\n",
       "      <td>uitspraak</td>\n",
       "      <td>['Strafrecht']</td>\n",
       "      <td>\\n\\narrestnummer: \\nparketnummer:\\t23-001835-0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18124 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id verdict_date publication_date verdict_type  \\\n",
       "0        ECLI-NL-RBNNE-2021-5018   2021-01-10       2021-11-23    uitspraak   \n",
       "1      ECLI-NL-RBZUT-2003-AH9598   2003-03-06       2003-09-07    uitspraak   \n",
       "2        ECLI-NL-RBZWB-2020-2646   2020-06-23       2020-06-23    uitspraak   \n",
       "3        ECLI-NL-GHAMS-2019-1601   2019-08-05       2019-07-26    uitspraak   \n",
       "4        ECLI-NL-GHAMS-2019-1602   2019-08-05       2019-07-26    uitspraak   \n",
       "...                          ...          ...              ...          ...   \n",
       "18119    ECLI-NL-RBAMS-2013-1294   2013-01-29       2013-08-10    uitspraak   \n",
       "18120      ECLI-NL-PHR-2020-1106   2020-11-24       2020-11-24    conclusie   \n",
       "18121    ECLI-NL-GHAMS-2017-2618   2017-06-29       2017-05-07    uitspraak   \n",
       "18122  ECLI-NL-RBAMS-2013-BZ0392   2013-01-29       2013-01-02    uitspraak   \n",
       "18123  ECLI-NL-GHAMS-2007-BB2447   2007-07-13       2007-08-29    uitspraak   \n",
       "\n",
       "      jurisdiction_type                                          case text  \n",
       "0        ['Strafrecht']  \\n\\nRECHTBANK NOORD-NEDERLAND\\nAfdeling strafr...  \n",
       "1        ['Strafrecht']  \\n\\nRECHTBANK ZUTPHEN\\nMeervoudige economische...  \n",
       "2        ['Strafrecht']  \\n\\nRECHTBANK ZEELAND-WEST-BRABANT\\n\\nStrafrec...  \n",
       "3        ['Strafrecht']  \\n\\nafdeling strafrecht\\nparketnummer: 23-0017...  \n",
       "4        ['Strafrecht']  \\n\\nafdeling strafrecht\\nparketnummer: 23-0017...  \n",
       "...                 ...                                                ...  \n",
       "18119    ['Strafrecht']  \\n\\nRECHTBANK AMSTERDAM\\n\\n\\nVONNIS\\n\\n \\n\\n13...  \n",
       "18120    ['Strafrecht']  \\n\\nPROCUREUR-GENERAAL\\n\\n\\nBIJ DE\\n\\n\\nHOGE R...  \n",
       "18121    ['Strafrecht']  \\n\\n\\nparketnummer: 23-001217-13\\ndatum uitspr...  \n",
       "18122    ['Strafrecht']  \\nRECHTBANK AMSTERDAM \\nVONNIS  \\n\\n13/529144-...  \n",
       "18123    ['Strafrecht']  \\n\\narrestnummer: \\nparketnummer:\\t23-001835-0...  \n",
       "\n",
       "[18124 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr|mr|mevr|mvr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|nl)\"\n",
    "articles = \"[artikel ][0-9][.][0-9]\"\n",
    "\n",
    "def split_into_sentences2(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(articles,\"[artikelnummer]\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])|\\n', text)\n",
    "    sentences = [x for x in sentences if len(x) > 1]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Two ways of splitting the documents:\n",
    "    For Word2Vec, we need sentences to be an array of words.\n",
    "    For the rest, just the sentence is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18124\n",
      "5107544\n"
     ]
    }
   ],
   "source": [
    "sentence_list_by_word = []\n",
    "sentence_list = []\n",
    "\n",
    "for i in range(len(merged_df)):\n",
    "    doc = merged_df.iloc[i]['case text']\n",
    "    sentences = split_into_sentences(doc)\n",
    "    sentence_list.append(sentences)\n",
    "    for j in sentences:\n",
    "        word_list = [x for x in j.lower().rstrip().replace('.', '').split(' ') if len(x)>0]\n",
    "        sentence_list_by_word.append(word_list)\n",
    "        \n",
    "print(len(sentence_list))\n",
    "print(len(sentence_list_by_word))\n",
    "# print(sentence_list[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Testers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in sentence_list[:1]:\n",
    "    for j in i:\n",
    "        print(j, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in sentence_list_by_word[:1]:\n",
    "    for j in i:\n",
    "        print(j, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_word2vec_model = Word2Vec(sentences=sentence_list_by_word, vector_size=100, window=5, min_count=1, workers=4)\n",
    "dutch_word2vec_model.save(\"word2vec_dutch_court_cases.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_word2vec_model = Word2Vec.load(\"word2vec_dutch_court_cases.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['export', 'import', 'invoer', 'doorvoer', 'exporteren', 'aanvoer', 'verkoop', 'gesmokkelde', 'verhandeling', 'handel', 'drugssmokkel', 'invoeren', 'versnijding', 'transporteren', 'fabricage', 'terugwinproces', 'leveranciers', 'producenten', 'cocaïnehandel', 'importeren', 'versnijden', 'smokkelen', 'organisatoren', 'overdracht', 'aankoop', 'levering', 'afhalers', 'bronlanden', 'doorlating', 'drugstransporten', 'doorlevering', '(invoer', 'uithalen', 'leverancier', 'productie/verwerking', 'in-/uitvoer', 'distributie', 'verwerking', 'doorverkoop', 'produktie', 'overdacht', 'straathandel', 'onderkennen/herkennen', 'georganiseerde', 'dealen', 'transporten', 'hennephandel', '(synthetische)', 'verspreiding', 'aankopen', 'vermenging', 'synthetische', 'kweek', 'toestroom', 'drugshandel', 'koeriers', 'verkooppunten', 'vervaardiging', 'verkopers', 'verwerkingsproces', 'leveranties', 'productie', 'vervoerden[h]', \"'dealpanden'\", 'grootschalige', 'binnensmokkelen', 'binnenbrengen', 'productieketen', 'kiloprijs', 'cocaïnesmokkel', '(internationale)', 'verhandelen', 'bronland', 'opzetten', 'inkoop', 'aanschaf', '\"verdwenen\"', 'strafrichtlijn', 'heroïnehandel', 'versnijdingenmiddel', 'hennepteelt', 'wederverkoop', 'afzet', 'binnenhalen', 'investeerders', 'afleveringen', 'leveringen', 'transport', 'wapentransporten', 'soft', 'zaten144', 'afzetmarkt', 'vervoer', 'bewerking', 'hennepkweek', 'produceren', 'bewerken/verwerken', 'voorraden', 'praktijken', 'groepering']\n"
     ]
    }
   ],
   "source": [
    "sims = dutch_word2vec_model.wv.most_similar('smokkel', topn=100)\n",
    "print([i[0] for i in sims])\n",
    "# print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of drugs, smuggle, quantity keywords with Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2vec_relevant_words(words, matches):\n",
    "    word2vec_list = []\n",
    "    for word in words:\n",
    "        results = dutch_word2vec_model.wv.most_similar(word, topn=100)\n",
    "        for i in results:\n",
    "            word2vec_list.append(i[0])\n",
    "            \n",
    "    word2vec_list = list(set([i for i in word2vec_list if word2vec_list.count(i)>matches]))\n",
    "    return word2vec_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drugs list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "['amfetamine;', '(met)amfetamine', 'cocaïne', 'oxazepam', 'mefedron', 'pure', 'xtc-tabletten', 'speed', 'mdma-poeder', 'metamfetamine', 'xtc-pillen', 'ketamine', 'marihuana', 'harddrugs', 'mdma-kristallen', 'cannabis', '2c-b', 'crystal', 'amfetamine', 'mdma/xtc', 'opium', 'amfetaminesulfaat', 'kristallen', 'cocaine', 'crack', 'methanol', 'pillen', 'amfetamine)', 'temazepam', 'hasjiesj', 'mdma;', 'manitol', 'methamfetamine', 'fenacetine', 'drugs', 'hash', 'amfetaminebase', 'zwavelzuur', 'methadon', 'paracetamol', 'amfetamineolie', 'eindproduct', 'hennep)', 'coke', 'xtc', 'lidocaïne', 'morfine', 'levamisol', 'poeder', 'mdma', 'pillen)', 'ecstasy', '(mdma)', 'platina', '34-methyleendioxymethamfetamine', 'oxycodon', 'ghb', 'mdma)', 'heroïne)', 'heroïne;', 'hasj', 'weed', 'cafeïne', 'gbl', 'hashish', 'sildenafil', 'hennep', 'heroïne', 'n-formylamfetamine', 'cocaïne;', 'cocaïne)', 'pep', 'mapa', 'amfetaminepasta', 'amfetaminen', 'azijnzuuranhydride', 'diazepam', 'lsd']\n"
     ]
    }
   ],
   "source": [
    "list_of_drugs = ['xtc', 'mdma', 'cocaine', 'wiet', 'speed', 'bmk', 'pmk']\n",
    "word2vec_drug_list = create_word2vec_relevant_words(list_of_drugs, 2)\n",
    "\n",
    "print(len(word2vec_drug_list))\n",
    "print(word2vec_drug_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smuggle keyword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "['aankoop', 'leverantie', 'doorlevering', 'produktie', 'importeren', 'in-/uitvoer', 'export', 'bewerking', 'handel', 'transporten', 'bronland', 'levering', 'smokkel', 'verhandeling', 'invoer', 'hennepkweek', 'vervoerden[h]', 'leverancier', 'productie', 'verkoop', '(invoer', 'versnijding', 'fabricage', 'aanvoer', 'cocaïnehandel', 'kweek', 'organisatoren', 'invoeren', 'gesmokkelde', 'productie/verwerking', 'import', 'afzet', 'binnenhalen', 'transporteren', 'uithalen', 'exporteren', 'hennepteelt', 'vervaardiging', 'doorvoer']\n"
     ]
    }
   ],
   "source": [
    "list_of_smuggle_words = ['smokkel', 'invoer', 'uitvoer', 'import', 'export', 'transport']\n",
    "word2vec_smuggle_list = create_word2vec_relevant_words(list_of_smuggle_words, 3)\n",
    "\n",
    "print(len(word2vec_smuggle_list))\n",
    "print(word2vec_smuggle_list)\n",
    "word2vec_smuggle_list = word2vec_smuggle_list + list_of_smuggle_words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantity keyword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "['gr', 'vaatjes', 'gram10', 'sealbags', 'papertrips', 'pillen/tabletten', 'wikkels', 'sibutramine', 'drums', 'km/h', 'xtc-pillen', 'verpakkingen', 'mg', 'sealtjes', 'gripzakken', 'gram”', 'km/u', 'gram;', 'pakketten', 'kilo)', 'stuks', '(xtc-)pillen', 'planten)', 'bolletjes', 'ug/1', 'kilo', 'promille', 'gram)', 'kilogram;', 'brokjes', 'pillen', 'kg)', 'emmers', 'tabletten', 'joints', 'zegels', 'zakjes', 'cm)', 'papiertjes', 'liter', 'potten', 'tabletten/pillen', 'blikken', 'ml', 'slikkersbollen', 'mg)', 'seals', 'mg/ml', 'm²', 'watt', 'pakjes', 'liter)', 'potjes', 'dozen', 'blokken', 'mdma-pillen', 'percent', 'db(a)', 'ton', 'doosjes', 'plakken', 'kg', 'kilo;', 'balen', 'gram13', 'm2', 'ponypacks', 'tonnen', 'pilletjes', 'ampullen', 'kilogram', 'bollen', 'kilogram)', 'cachetjes', 'milliliter', 'snowseals', 'mg/m2', 'pallets', 'bakjes', 'km', 'gripzakjes', 'blokjes', '%', 'flessen', 'ng/ml', 'kg”', 'flacons']\n"
     ]
    }
   ],
   "source": [
    "list_of_quantity_words = ['tabletten', 'kilo', 'gram', 'pakketten']\n",
    "word2vec_quantity_list = create_word2vec_relevant_words(list_of_quantity_words, 1)\n",
    "\n",
    "print(len(word2vec_quantity_list))\n",
    "print(word2vec_quantity_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "['frankrijk', 'hamburg', 'engeland', 'canada', 'curaçao', 'bulgarije', 'alicante', 'thailand', 'marokko', 'italië', 'tsjechië', 'servië', 'sevilla', 'nigeria', 'syrië', 'guayaquil', 'noorwegen', 'tanger', 'mexico', 'antwerpen', 'trinidad', 'portugal', 'venezuela', 'dubai', 'australië', 'afrika', 'londen', 'polen', 'brussel', 'paramaribo', 'chili', 'schotland', 'iran', 'natal', 'hongarije', 'lissabon', 'bosnië', 'oostenrijk', 'ecuador', 'groot-brittannië', 'oekraïne', 'istanbul', 'brazilië', 'argentinië', 'roemenië', 'turkije', 'jamaica', 'kroatië', 'kenia', 'parijs', 'ierland', 'denemarken', 'rabat', 'china', 'panama', 'ghana', 'slowakije', 'madrid', 'boedapest', 'belgie', 'malaga', 'suriname', 'zwitserland', 'congo', 'costa', 'rica', 'barcelona', 'luxemburg', 'kinshasa', 'griekenland', 'irak', 'peru', 'lima', 'belgië', 'berlijn', 'amerika', 'nederland', 'dominicaanse', 'europa', 'pakistan', 'spanje', 'zuid-amerika', '[land]', 'zweden']\n"
     ]
    }
   ],
   "source": [
    "list_of_countries = ['duitsland', 'colombia']\n",
    "word2vec_country_list = create_word2vec_relevant_words(list_of_countries, 1)\n",
    "\n",
    "print(len(word2vec_country_list))\n",
    "print(word2vec_country_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create SpaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "# !python -m spacy download nl_core_news_md\n",
    "nlp = spacy.load('nl_core_news_md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_file = open(\"drugs list.txt\", \"r\", encoding='utf-8')\n",
    "my_file = my_file.readlines()\n",
    "drugs_list = []\n",
    "for i in my_file:\n",
    "    drugs_list.append(i.replace('\\n', ''))\n",
    "\n",
    "my_file = open(\"countries list.txt\", \"r\", encoding='utf-8')\n",
    "my_file = my_file.readlines()\n",
    "countries_list = []\n",
    "for i in my_file:\n",
    "    countries_list.append(i.replace('\\n', ''))\n",
    "countries_list = countries_list + word2vec_country_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "complete_drugs_list = list(set(drugs_list + word2vec_drug_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def configure_spacy_model():\n",
    "    # Create dict of drug pattern and quantity pattern\n",
    "    pattern_list = []\n",
    "    \n",
    "    drugs_ent_list = []\n",
    "    for i in complete_drugs_list:\n",
    "        pattern_list.append({\"label\": \"DRUG\", \"pattern\": [{\"lower\": i}]})\n",
    "    \n",
    "#     quantity_rule = {\"label\": \"QUANTITY\", \"pattern\": [{\"IS_DIGIT\": True}, {\"LOWER\": \"gram\"}]}\n",
    "#     pattern_list.append(quantity_rule)\n",
    "    for i in word2vec_quantity_list:\n",
    "        pattern_list.append({\"label\": \"QUANTITY\", \"pattern\": [{\"IS_DIGIT\": True}, {\"LOWER\": i}]})\n",
    "        pattern_list.append({\"label\": \"QUANTITY\", \"pattern\": [{\"ENT_TYPE\": \"CARDINAL\"}, {\"LOWER\": i}]})\n",
    "    \n",
    "    for i in countries_list:\n",
    "        pattern_list.append({\"label\": \"GPE\", \"pattern\": [{\"lower\": i.replace(' ', '').lower()}]})\n",
    "    \n",
    "    # Add drug and quantity rules to the model\n",
    "    config = {\n",
    "   \"phrase_matcher_attr\": None,\n",
    "   \"validate\": True,\n",
    "   \"overwrite_ents\": True,\n",
    "   \"ent_id_sep\": \"||\",\n",
    "    }\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", config=config)\n",
    "\n",
    "    #List of Entities and Patterns\n",
    "#     patterns = drugs_ent_list\n",
    "    ruler.add_patterns(pattern_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "configure_spacy_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select cases and chunks to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence_list) == len(merged_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every case, split the sentences. If a sentence in a case contains a drug, a smuggle word, and a location: keep chunk and save to trafficking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_chunk_list = []\n",
    "ecli_list = []\n",
    "\n",
    "for index, case in enumerate(sentence_list):\n",
    "    chunk_list = []\n",
    "    trafficking_related = False\n",
    "    for chunk in case:\n",
    "        if any(drug in chunk for drug in word2vec_drug_list) and any(smuggle_word in chunk for smuggle_word in word2vec_smuggle_list):\n",
    "            ents = nlp(chunk).ents\n",
    "            if any(ent.label_ == \"GPE\" or ent.label_ == \"LOC\" for ent in ents):\n",
    "                trafficking_related = True\n",
    "                chunk_list.append(chunk)\n",
    "    if trafficking_related:\n",
    "        relevant_chunk_list.append(chunk_list)\n",
    "        ecli_list.append(merged_df.iloc[index]['id'].replace('-', ':'))\n",
    "\n",
    "trafficking_df = pd.DataFrame({'id': pd.Series(ecli_list), 'chunks': pd.Series(relevant_chunk_list)})                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4670 cases kept from original 18124 cases.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(trafficking_df)} cases kept from original {len(merged_df)} cases.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create rule-based NER & POS tagging model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_chunk_info(txt):\n",
    "    source_country = None\n",
    "    total_info = []\n",
    "    for token in nlp(txt):\n",
    "        info = {}\n",
    "        drug_info = {}\n",
    "        countries = []\n",
    "        \n",
    "        if token.ent_type_ == \"DRUG\":\n",
    "            info = {\"drug\": token.text}\n",
    "            \n",
    "            ## Get source and destination\n",
    "            for ancestor in token.ancestors:\n",
    "                for nephew in ancestor.children:\n",
    "                    if nephew.ent_type_ == \"GPE\" or nephew.ent_type_ == \"LOC\":\n",
    "                        countries.append(nephew)\n",
    "                        for child in nephew.children:\n",
    "                            if child.dep_ == \"conj\" and child.ent_type_ == \"GPE\" or child.ent_type_ == \"LOC\":\n",
    "                                countries.append(child.text)\n",
    "                            elif child.pos_ == \"ADP\" and child.dep_ == \"case\":\n",
    "                                adj = child.text\n",
    "            if len(countries) > 0 :\n",
    "                try:\n",
    "                    info[adj] = countries\n",
    "                except:\n",
    "                    info['land'] = countries\n",
    "                        \n",
    "            ## Get volume\n",
    "            for ancestors in token.ancestors:\n",
    "                for nephew in ancestors.children:\n",
    "                    if nephew.ent_type_ == \"QUANTITY\" or nephew.ent_type_ == \"CARDINAL\":\n",
    "                        for second_nephew in nephew.children:\n",
    "                            if second_nephew.is_digit != nephew.is_digit:\n",
    "                                if second_nephew.is_digit:\n",
    "                                    info['volume'] = second_nephew.text\n",
    "                                    info['volume_type'] = nephew.text\n",
    "                                else:\n",
    "                                    info['volume'] = nephew.text\n",
    "                                    info['volume_type'] = second_nephew.text\n",
    "            if 'volume' not in info:\n",
    "                for child in token.children:\n",
    "                    if (child.dep_ == \"det\" and child.like_num) or (child.dep_ == \"nummod\"):\n",
    "                        info['volume'] = child.text\n",
    "                                \n",
    "        if len(info) > 1:\n",
    "#             print(info)\n",
    "            total_info.append(info)\n",
    "    return total_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get linguistic distance between token a and token b. After iter 10 it is deemed a too far distance.\n",
    "def get_linguistic_distance(a, b):\n",
    "    tokens_to_consider = [b]\n",
    "    found = False\n",
    "    iters = 0\n",
    "    while not found:\n",
    "        for token in tokens_to_consider:\n",
    "            tokens_to_add = []\n",
    "            for ancestor in token.ancestors:\n",
    "                if ancestor not in tokens_to_add and ancestor not in tokens_to_consider:\n",
    "                    tokens_to_add.append(ancestor)\n",
    "            for child in token.children:\n",
    "                if child not in tokens_to_add and child not in tokens_to_consider:\n",
    "                    tokens_to_add.append(child)\n",
    "            tokens_to_consider = tokens_to_consider + tokens_to_add\n",
    "        for x in tokens_to_consider:\n",
    "            if a.orth == x.orth:\n",
    "                found = True\n",
    "        iters += 1\n",
    "        if iters == 10:\n",
    "            found = True\n",
    "    return iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adposition_from_loc(token):\n",
    "    for child in token.children:\n",
    "        if child.pos_ == \"ADP\" and child.dep_ == \"case\":\n",
    "            return child.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunk_info(txt):\n",
    "    result = {}\n",
    "#     print('START CHUNK')\n",
    "    for token in nlp(txt):\n",
    "        if token.ent_type_ == \"DRUG\":\n",
    "#             print(f\" \\n Extracting info for {token.text}.\")\n",
    "            drug_info = extract_info_from_drug(token, txt)\n",
    "            result[token.text] = drug_info\n",
    "    return result\n",
    "            \n",
    "            \n",
    "def extract_info_from_drug(drug, txt):\n",
    "    volumes = []\n",
    "    locations = {}\n",
    "    irrelevant_locations = []\n",
    "    for token in nlp(txt):\n",
    "        \n",
    "        # Extract countries\n",
    "        if token.ent_type_ == \"GPE\" or token.ent_type_ == \"LOC\":\n",
    "            dist = get_linguistic_distance(drug, token)\n",
    "            if dist < 10:\n",
    "                adj = get_adposition_from_loc(token)\n",
    "#                 print(f\"    {adj}: {token.text}, dist: {dist}, conj: {token.conjuncts}\")\n",
    "                locs = [token.text]\n",
    "                for loc in token.conjuncts:\n",
    "                    locs.append(loc.text)\n",
    "                locations[adj] = locs\n",
    "            else:\n",
    "                irrelevant_locations.append(token.text)\n",
    "#                 print(f\"{token.text} is irrelevant.\")\n",
    "        \n",
    "        # Extract volume\n",
    "        if token.ent_type_ == \"QUANTITY\":\n",
    "            volume = {}\n",
    "            dist = get_linguistic_distance(drug, token)\n",
    "            second_token = \"\"\n",
    "            if dist < 10:\n",
    "                quantity = {}\n",
    "                for ancestor in token.ancestors:\n",
    "                    if ancestor.ent_type_ == \"QUANTITY\":\n",
    "                        second_token = ancestor\n",
    "                for child in token.children:\n",
    "                    if child.ent_type_ == \"QUANTITY\":\n",
    "                        second_token = child\n",
    "                \n",
    "                ## Decide volume and volume_type\n",
    "                if nlp(token.text)[0].ent_type_ == \"CARDINAL\":\n",
    "                    volume['volume'] = token.text\n",
    "                    volume['volume_type'] = second_token.text\n",
    "                elif nlp(second_token.text)[0].ent_type_ == \"CARDINAL\":\n",
    "                    volume['volume'] = second_token.text\n",
    "                    volume['volume_type'] = token.text\n",
    "                \n",
    "                #Only append when not already in volumes\n",
    "                if volume not in volumes:\n",
    "                    volumes.append(volume)\n",
    "#             else:\n",
    "#                 print(f\"{token.text} is irrelevant.\")\n",
    "                \n",
    "        \n",
    "        \n",
    "            \n",
    "#     print(volumes)\n",
    "\n",
    "    result = {}\n",
    "    if bool(locations):\n",
    "        result['locations'] = locations\n",
    "    if len(volumes) > 0:\n",
    "        result[\"volume\"] = volumes\n",
    "    if len(irrelevant_locations) > 0:\n",
    "        result[\"irrelevant_locations\"] = irrelevant_locations\n",
    "    \n",
    "    return result\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [160]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocations\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m info[drugtype]:\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;28mprint\u001b[39m(info)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mget_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mECLI:NL:GHARN:2004:AR3598\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [160]\u001b[0m, in \u001b[0;36mget_summary\u001b[1;34m(ecli)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_summary\u001b[39m(ecli):\n\u001b[1;32m----> 2\u001b[0m     rel_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mtrafficking_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrafficking_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mecli\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchunks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rel_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks in ecli: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mecli\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m rel_chunks:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def get_summary(ecli):\n",
    "    rel_chunks = trafficking_df[trafficking_df['id'] == ecli]['chunks'].tolist()[0]\n",
    "    print(f\"{len(rel_chunks)} chunks in ecli: {ecli}\")\n",
    "    for chunk in rel_chunks:\n",
    "        info = extract_chunk_info(chunk)\n",
    "        drugtype = next(iter(info))\n",
    "        if 'locations' in info[drugtype]:\n",
    "            print(info)\n",
    "        \n",
    "get_summary('ECLI:NL:GHARN:2004:AR3598')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 chunks in ecli: ECLI:NL:RBMAA:2010:BM8942\n",
      "{'amfetamine': {'locations': {'te': ['Landgraaf']}, 'volume': [{}]}}\n",
      "{'wiet': {'locations': {'naar': ['Duitsland']}}, 'amfetamine': {'locations': {'naar': ['Duitsland']}}}\n",
      "{'amfetamine': {'locations': {None: ['Bonn']}, 'volume': [{'volume': 'vijftig', 'volume_type': 'kilo'}]}}\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [159]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m from_case:\n\u001b[0;32m      8\u001b[0m     ecli \u001b[38;5;241m=\u001b[39m trafficking_df\u001b[38;5;241m.\u001b[39miloc[index][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mget_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mecli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [158]\u001b[0m, in \u001b[0;36mget_summary\u001b[1;34m(ecli)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m rel_chunks:\n\u001b[0;32m      5\u001b[0m     info \u001b[38;5;241m=\u001b[39m extract_chunk_info(chunk)\n\u001b[1;32m----> 6\u001b[0m     drugtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocations\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m info[drugtype]:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(info)\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from_case = 40\n",
    "to_case = 60\n",
    "\n",
    "result_list = []\n",
    "\n",
    "for index in range(len(trafficking_df[:from_case+to_case])):\n",
    "    if index >= from_case:\n",
    "        ecli = trafficking_df.iloc[index]['id']\n",
    "        get_summary(ecli)\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'locations': {'te': ['Landgraaf']}, 'volume': [{}], 'amfetamine': {...}}\n",
      "{'locations': {'naar': ['Duitsland']}, 'amfetamine': {...}}\n",
      "{'locations': {None: ['Bonn']}, 'volume': [{'volume': 'vijftig', 'volume_type': 'kilo'}], 'amfetamine': {...}}\n",
      "{}\n",
      "{'locations': {'naar': ['Duitsland']}, 'amfetamine': {...}}\n",
      "{'locations': {'te': ['Landgraaf']}, 'amfetamine': {...}}\n",
      "{'locations': {'naar': ['Duitsland']}, 'wiet': {...}}\n",
      "{'locations': {'te': ['Landgraaf']}, 'amfetamine': {...}}\n",
      "{'locations': {'te': ['Landgraaf']}, 'amfetamine': {...}}\n",
      "{'locations': {'te': ['Landgraaf']}, 'volume': [{}], 'amfetamine': {...}}\n",
      "{'locations': {'naar': ['Duitsland']}, 'amfetamine': {...}}\n",
      "{'locations': {'te': ['Landgraaf']}, 'amfetamine': {...}}\n",
      "{'locations': {'in': ['Duitsland']}, 'amfetamine': {...}}\n",
      "{'volume': [{'volume': 'vijftig', 'volume_type': 'kilo'}], 'amfetamine': {...}}\n",
      "{}\n",
      "{'locations': {'naar': ['Duitsland']}, 'amfetamine': {...}}\n",
      "{'locations': {'te': ['Landgraaf']}, 'amfetamine': {...}}\n",
      "{'locations': {'te': ['Landgraaf']}, 'amfetamine': {...}}\n",
      "{'locations': {'naar': ['Duitsland']}, 'volume': [{'volume': '85', 'volume_type': 'kilo'}], 'amfetamine': {...}}\n",
      "{'locations': {'te': ['Vlierden'], None: ['Deurne']}, 'amfetamine': {...}}\n",
      "{'locations': {'te': ['Vlierden'], None: ['Deurne']}, 'amfetamine': {...}}\n",
      "{'locations': {'te': ['Vlierden'], None: ['Deurne']}, 'amfetamine': {...}}\n",
      "{'locations': {'te': ['Vlierden'], None: ['Deurne']}, 'amfetamine': {...}}\n",
      "{'locations': {'te': ['Vlierden'], None: ['Deurne']}, 'amfetamine': {...}}\n",
      "{'locations': {'van': ['rechtbank'], None: ['Oost-Brabant']}, 'volume': [{'volume': '65', 'volume_type': 'gram'}, {}, {'volume': '1112', 'volume_type': 'gram'}, {'volume': '7', 'volume_type': 'gram'}, {'volume': '36', 'volume_type': 'gram'}], 'hennep': {...}}\n",
      "{'locations': {'te': ['Oss'], 'in': ['Nederland']}, 'volume': [{'volume': '436', 'volume_type': 'kg'}, {}, {'volume': '30', 'volume_type': 'kg'}, {'volume': '30', 'volume_type': 'liter'}], 'amfetamine': {...}}\n",
      "{'volume': [{'volume': '436', 'volume_type': 'kg'}, {}, {'volume': '30', 'volume_type': 'kg'}, {'volume': '30', 'volume_type': 'liter'}], 'amfetamine': {...}}\n",
      "{'volume': [{'volume': '436', 'volume_type': 'kg'}], 'amfetamine': {...}}\n",
      "{'locations': {'naar': ['buitenland']}, 'amfetamine': {...}}\n",
      "{'locations': {None: ['Vasstaat']}, 'amfetamine': {...}}\n",
      "{}\n",
      "{'cocaïne': {...}}\n",
      "{}\n",
      "{'locations': {'van': ['Curaçao']}, 'cocaïne': {...}}\n",
      "{'locations': {'van': ['Curaçao']}, 'cocaïne': {...}}\n",
      "{'locations': {'te': ['Leende']}, 'amfetamine': {...}}\n",
      "{'locations': {None: ['bevorderingshandelingen', 'voorbereidings-']}, 'MDMA': {...}}\n",
      "{'locations': {'met': ['Leuckartmethode']}, 'BMK': {...}}\n",
      "{'locations': {'met': ['Leuckartmethode']}, 'PMK': {...}}\n",
      "{}\n",
      "{'locations': {'in': ['Engeland']}, 'volume': [{'volume': '60', 'volume_type': 'kilogram'}, {'volume': '80', 'volume_type': 'kilogram'}], 'hasj': {...}}\n",
      "{}\n",
      "{'hasj': {...}}\n",
      "{'locations': {'te': ['Amsterdam', 'gemeente'], 'in': ['gemeente', 'Amsterdam'], None: ['Haarlemmermeer'], 'van': ['Nederland'], 'vanuit': ['Zuid-Amerika']}, 'volume': [{'volume': '116', 'volume_type': 'kilogram'}], 'cocaïne': {...}}\n",
      "{'locations': {'binnen': ['Nederland']}, 'cocaïne': {...}}\n",
      "{'locations': {'van': ['Nederland'], 'in': ['art.'], None: ['Ow'], 'binnen': ['Nederland']}, 'volume': [{'volume': '116', 'volume_type': 'kilogram'}], 'cocaïne': {...}}\n",
      "{'locations': {'te': ['Amsterdam', 'gemeente'], 'in': ['gemeente', 'Amsterdam'], None: ['Haarlemmermeer'], 'van': ['Nederland'], 'vanuit': ['Zuid-Amerika']}, 'volume': [{'volume': '116', 'volume_type': 'kilogram'}], 'cocaïne': {...}}\n",
      "{'locations': {'te': ['Bergschenhoek']}, 'MDMA': {...}}\n",
      "{'locations': {'in': ['Rotterdam', 'Bergschenhoek']}, 'hasjiesj': {...}}\n",
      "{'locations': {'naar': ['Nederland']}, 'cocaïne': {...}}\n",
      "{'locations': {'van': ['drugs']}, 'lidocaïne': {...}}\n",
      "{'locations': {'van': ['drugs']}, 'lidocaïne': {...}}\n",
      "{'locations': {'in': ['Nederland']}, 'volume': [{'volume': '11', 'volume_type': 'kilo'}], 'cocaïne': {...}}\n",
      "{'locations': {None: ['eenvoorhanden']}, 'heroïne': {...}}\n",
      "{'locations': {'te': ['Zutphen']}, 'hennep': {...}}\n",
      "{}\n",
      "{'locations': {'te': ['Amsterdam'], 'in': ['Nederland']}, 'speed/amfetamine': {...}}\n",
      "{'locations': {'in': ['Wernhout']}, 'XTC-pillen': {...}}\n",
      "{'locations': {'aan': ['Ettenseweg'], 'te': ['Rijsbergen']}, 'XTC-pillen': {...}}\n",
      "{'locations': {None: ['N-ethyl', 'MDA', 'MDMA', 'Amfetamine']}, 'Amfetamine': {...}}\n",
      "{'locations': {None: ['N-ethyl', 'MDA', 'MDMA', 'Amfetamine']}, 'Amfetamine': {...}}\n",
      "{'locations': {None: ['N-ethyl', 'MDA', 'MDMA', 'Amfetamine']}, 'Amfetamine': {...}}\n",
      "{'locations': {None: ['N-ethyl', 'Amfetamine', 'MDA', 'MDMA']}, 'MDMA': {...}}\n",
      "{'locations': {None: ['N-ethyl', 'MDA', 'MDMA', 'Amfetamine']}, 'Amfetamine': {...}}\n",
      "{'XTC-pillen': {...}}\n",
      "{'locations': {'in': ['Nederland']}, 'amfetamine': {...}}\n",
      "{'locations': {'in': ['Nederland']}, 'harddrugs': {...}}\n",
      "{'harddrugs': {...}}\n",
      "{'locations': {None: ['poedersporen', 'sealtjes']}, 'amfetamine': {...}}\n",
      "{'locations': {'van': ['€']}, 'cocaïne': {...}}\n",
      "{'locations': {None: ['slammen', 'hebben']}, 'methamfetamine': {...}}\n",
      "{'locations': {'in': ['Nederland']}, 'volume': [{'volume': '24', 'volume_type': 'kilogram'}], 'cocaïne': {...}}\n",
      "{'locations': {'naar': ['Duitsland']}, 'cocaïne': {...}}\n",
      "{'amfetaminepasta': {...}}\n",
      "{'locations': {'in': ['Roermond']}, 'cocaïne': {...}}\n",
      "{'locations': {'naar': ['Roermond'], 'in': ['Roermond']}, 'cocaïne': {...}}\n",
      "{'locations': {'van': ['Nederland']}, 'harddrugs': {...}}\n",
      "{'MDMA': {...}}\n",
      "{'locations': {'in': ['buitenland', 'Nederland']}, 'aceton': {...}}\n",
      "{'locations': {None: ['het/die']}, 'harddrugs': {...}}\n",
      "{'locations': {None: ['piperonal']}, 'MDMA': {...}}\n",
      "{'locations': {'naar': ['Tilburg']}, 'speed': {...}}\n",
      "{'locations': {None: ['België']}, 'harddrugs': {...}}\n",
      "{'locations': {None: ['België']}, 'harddrugs': {...}}\n",
      "{'harddrugs': {...}}\n",
      "{'locations': {'in': ['België']}, 'amfetamine': {...}}\n",
      "{'locations': {'van': ['Nederland']}, 'harddrugs': {...}}\n",
      "{'locations': {None: ['Breda']}, 'MDMA': {...}}\n",
      "{'locations': {'in': ['Nederland']}, 'harddrugs': {...}}\n",
      "{'harddrugs': {...}}\n",
      "{'locations': {'te': ['Amsterdam'], 'in': ['Nederland'], 'van': ['Nederland']}, 'volume': [{'volume': '3', 'volume_type': 'kilogram'}], 'cocaïne': {...}}\n",
      "{'locations': {'vanuit': ['Brazilië']}, 'cocaïnetransport': {...}}\n",
      "{'locations': {'te': ['Zaandam'], 'in': ['Nederland']}, 'cocaïne': {...}}\n",
      "{'locations': {'in': ['Zaandam']}, 'harddrugs': {...}}\n",
      "{'XTC': {...}}\n",
      "{'locations': {'in': ['Maastricht']}, 'hennep': {...}}\n",
      "{'locations': {'naar': ['buitenland']}, 'hennep': {...}}\n"
     ]
    }
   ],
   "source": [
    "for i in result_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
